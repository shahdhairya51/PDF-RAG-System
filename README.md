# üîç PDF Semantic QA Pipeline with LangChain, FAISS, and LlamaCpp

A production-ready semantic question-answering system that processes PDF documents (text, tables, images) and enables intelligent Q&A using locally-hosted LLMs. Built with LangChain, FAISS, and optimized for GPU acceleration.

## üöÄ Features

- **Multi-modal PDF Processing**: Extracts text, tables, and images from PDFs
- **Semantic Vector Search**: Uses FAISS with sentence-transformers for dense embeddings
- **Local LLM Integration**: Leverages Qwen2.5-Omni-7B with GPU acceleration via LlamaCpp
- **Advanced Retrieval**: MMR-based chunk ranking for diverse context
- **Optimized QA Pipeline**: Custom prompt templates with answer post-processing
- **Production Ready**: Handles real-world PDFs with robust error handling

## üõ†Ô∏è Tech Stack

| Component        | Tool / Library                        |
|------------------|----------------------------------------|
| PDF Parsing       | `PyMuPDF`, `pdfplumber`               |
| Data Handling     | `pandas`, `json`, `pathlib`           |
| Embeddings        | `sentence-transformers`               |
| Vector Search     | `FAISS`                               |
| LLM Inference     | `LlamaCpp`, `Qwen2.5-Omni-7B-GGUF`    |
| Prompt Chains     | `LangChain`                           |
| Model Downloading | `huggingface_hub`                     |

## üö¶ Quick Start

```bash
# Clone and install
git clone https://github.com/yourusername/pdf-semantic-qa.git
cd pdf-semantic-qa
pip install -r requirements.txt

# Set up HuggingFace authentication
huggingface-cli login

# Place your PDF in data/thesis.pdf and run
python main.py
```

## üìã Requirements

```txt
PyMuPDF
pdfplumber
pandas
langchain
sentence-transformers
faiss-cpu
llama-cpp-python
huggingface_hub
```

## üéØ How It Works

### 1. **PDF Ingestion**
- Extracts text per page ‚Üí `data/text/page_XXX.txt`
- Tables as CSV ‚Üí `data/tables/table_XXX_0.csv`
- Images as PNG ‚Üí `data/images/image_XXX_0.png`
- Metadata saved in `data/metadata.json`

### 2. **Vector Store Creation**
- Text chunked using `RecursiveCharacterTextSplitter`
- Tables flattened into sentence-like rows
- Vectorized using `sentence-transformers/all-MiniLM-L6-v2`
- Indexed with FAISS for fast similarity search

### 3. **LLM Setup**
```python
llm = LlamaCpp(
    model_path="models/Qwen2.5-Omni-7B/Qwen2.5-Omni-7B.Q4_K_S.gguf",
    n_ctx=4096,
    n_threads=8,
    n_gpu_layers=12,        # GPU acceleration
    temperature=0.1,        # Focused responses
    max_tokens=300,
    top_p=0.9,
    repeat_penalty=1.1
)
```

### 4. **Intelligent Q&A**
```python
# Ask questions about your document
res = qa({"query": "What are the best performing models and their accuracy?"})
cleaned_answer = clean_answer(res["result"])
print(cleaned_answer)

# View source documents
for doc in res["source_documents"]:
    print(f"Page {doc.metadata['page']}: {doc.metadata['source']}")
```

## üîç Example Usage

```python
# Research Analysis
query = "What are the key findings and their accuracy metrics?"

# Technical Details  
query = "Which models performed best and why?"

# Data Extraction
query = "Extract all performance metrics from tables"

# Sample Output:
# "The XYZNet model achieved the highest accuracy of 96.2% on the benchmark dataset."
```

## üìä Performance Highlights

- **Processing Speed**: ~2-3 pages/second
- **Memory Usage**: ~6GB GPU, ~4GB RAM
- **Query Response Time**: ~2-5 seconds
- **Model Size**: 4.37GB (4-bit quantized)

## üé® Key Technical Features

**Smart Answer Processing**
```python
def clean_answer(answer):
    """Filters verbose LLM responses for concise answers"""
    lines = answer.split('\n')
    for line in lines:
        line = line.strip()
        if line and not any(phrase in line.lower() for phrase in 
                          ['note:', 'disclaimer:', 'assumption']):
            return line
    return answer.strip()
```

**Advanced Retrieval**
- MMR (Max Marginal Relevance) for diverse context
- Configurable top-k and similarity thresholds
- Table-aware processing with row-level indexing

**GPU Optimization**
- Configurable layer offloading
- Memory-efficient 4-bit quantization
- CUDA acceleration support

## üèÜ Why This Project Stands Out

- **Production-Ready**: Handles complex PDFs with robust error handling
- **Scalable**: Designed for enterprise-level document processing
- **Optimized**: GPU acceleration and memory-efficient implementations
- **Comprehensive**: End-to-end pipeline from PDF ingestion to intelligent Q&A
- **Modern Stack**: Uses latest LangChain, FAISS, and local LLM technologies


## üìû Contact

**Dhairya** - [shahdhairya51@gmail.com](mailto:shahdhairya51@gmail.com)

---

*Built with ‚ù§Ô∏è for intelligent document processing*
